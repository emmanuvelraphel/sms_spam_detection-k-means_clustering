{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv('../data/raw/raw_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMS_id</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\tGo until jurong point, crazy.. Available on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\\tOk lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\\tFree entry in 2 a wkly comp to win FA Cup f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\tU dun say so early hor... U c already then ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\\tNah I don't think he goes to usf, he lives ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SMS_id                                                SMS\n",
       "0       1   \\tGo until jurong point, crazy.. Available on...\n",
       "1       2                  \\tOk lar... Joking wif u oni...\\n\n",
       "2       3   \\tFree entry in 2 a wkly comp to win FA Cup f...\n",
       "3       4   \\tU dun say so early hor... U c already then ...\n",
       "4       5   \\tNah I don't think he goes to usf, he lives ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      "SMS_id    5574 non-null int64\n",
      "SMS       5574 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset.rename(columns={\"SMS_id\": \"id\",\"SMS\":\"sms\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\tGo until jurong point, crazy.. Available on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\\tOk lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\\tFree entry in 2 a wkly comp to win FA Cup f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\tU dun say so early hor... U c already then ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\\tNah I don't think he goes to usf, he lives ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                sms\n",
       "0   1   \\tGo until jurong point, crazy.. Available on...\n",
       "1   2                  \\tOk lar... Joking wif u oni...\\n\n",
       "2   3   \\tFree entry in 2 a wkly comp to win FA Cup f...\n",
       "3   4   \\tU dun say so early hor... U c already then ...\n",
       "4   5   \\tNah I don't think he goes to usf, he lives ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tGo until jurong point, crazy.. Available on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tOk lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tFree entry in 2 a wkly comp to win FA Cup f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tU dun say so early hor... U c already then ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tNah I don't think he goes to usf, he lives ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sms\n",
       "0   \\tGo until jurong point, crazy.. Available on...\n",
       "1                  \\tOk lar... Joking wif u oni...\\n\n",
       "2   \\tFree entry in 2 a wkly comp to win FA Cup f...\n",
       "3   \\tU dun say so early hor... U c already then ...\n",
       "4   \\tNah I don't think he goes to usf, he lives ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = raw_dataset.drop(['id'],axis=1,inplace=False)\n",
    "raw_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe of raw dataset which we can use for preprocesing.\n",
    "* We will use a sample from this dataframe for testing dataprocessing and feature extraction and then we will use the full dataframe for final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sample Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      \\tGo until jurong point, crazy.. Available on...\n",
      "1                     \\tOk lar... Joking wif u oni...\\n\n",
      "2      \\tFree entry in 2 a wkly comp to win FA Cup f...\n",
      "3      \\tU dun say so early hor... U c already then ...\n",
      "4      \\tNah I don't think he goes to usf, he lives ...\n",
      "5      \\tFreeMsg Hey there darling it's been 3 week'...\n",
      "6      \\tEven my brother is not like to speak with m...\n",
      "7      \\tAs per your request 'Melle Melle (Oru Minna...\n",
      "8      \\tWINNER!! As a valued network customer you h...\n",
      "9      \\tHad your mobile 11 months or more? U R enti...\n",
      "10     \\tI'm gonna be home soon and i don't want to ...\n",
      "11     \\tSIX chances to win CASH! From 100 to 20,000...\n",
      "12     \\tURGENT! You have won a 1 week FREE membersh...\n",
      "13     \\tI've been searching for the right words to ...\n",
      "14              \\tI HAVE A DATE ON SUNDAY WITH WILL!!\\n\n",
      "15     \\tXXXMobileMovieClub: To use your credit, cli...\n",
      "16                       \\tOh k...i'm watching here:)\\n\n",
      "17     \\tEh u remember how 2 spell his name... Yes i...\n",
      "18     \\tFine if thats the way u feel. Thats the w...\n",
      "19     \\tEngland v Macedonia - dont miss the goals/t...\n",
      "Name: sms, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# We use this dataframe for testing\n",
    "test_df = raw_dataset[:20]\n",
    "print(test_df['sms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing unnecessary punctuation, tags\n",
    "* Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic\n",
    "* Tokenization — convert sentences to words\n",
    "* Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
    "* Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean/Normalize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/emmanuvel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/emmanuvel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmanuvel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.functions import preprocessor\n",
    "test_cleaned_dataset = test_df['sms'].apply(preprocessor.preprocess).to_frame()  #preprocess will return tokenized text after cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  sms\n",
      "0   [jurong, point, crazy.., Available, bugis, gre...\n",
      "1                   [lar, ..., Joking, wif, oni, ...]\n",
      "2   [Free, entry, wkly, comp, win, Cup, final, tkt...\n",
      "3      [dun, say, early, hor, ..., already, say, ...]\n",
      "4   [Nah, n't, think, goes, usf, lives, around, th...\n",
      "5   [FreeMsg, Hey, darling, week, word, back, like...\n",
      "6   [Even, brother, like, speak, They, treat, like...\n",
      "7   [per, request, 'Melle, Melle, Oru, Minnaminung...\n",
      "8   [WINNER, valued, network, customer, selected, ...\n",
      "9   [Had, mobile, months, entitled, Update, latest...\n",
      "10  [gon, home, soon, n't, want, talk, stuff, anym...\n",
      "11  [SIX, chances, win, CASH, From, 100, 20,000, p...\n",
      "12  [URGENT, You, week, FREE, membership, 100,000,...\n",
      "13  ['ve, searching, right, words, thank, breather...\n",
      "14                   [HAVE, DATE, SUNDAY, WITH, WILL]\n",
      "15  [XXXMobileMovieClub, use, credit, click, WAP, ...\n",
      "16                                    [..., watching]\n",
      "17  [remember, spell, name, ..., Yes, naughty, mak...\n",
      "18         [Fine, thats, way, feel, Thats, way, gota]\n",
      "19  [England, Macedonia, dont, miss, goals/team, n...\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_dataset = test_df['sms'].apply(preprocessor.get_tokenized_text).to_frame()  #preprocess will return tokenized text after cleaning \n",
    "print(test_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = raw_dataset['sms'].apply(preprocessor.preprocess).to_frame()  #preprocess will return tokenized text after cleaning \n",
    "tokenized_dataset = raw_dataset['sms'].apply(preprocessor.get_tokenized_text).to_frame() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The mapping of textual data to real valued vectors is called feature extraction.\n",
    "* One of the simplest techniques to numerically represent text is BAG OF WORDS (BOW).\n",
    "* We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or    document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Vocabulary\n",
    "from src.functions import feature_extractor\n",
    "test_vocabulary, test_vectorized_list = feature_extractor.get_vocabulary(test_tokenized_dataset['sms']) # get_vocabualry() will return a dict of vocabulary available across whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jurong': 0, 'point': 1, 'crazy..': 2, 'Available': 3, 'bugis': 4, 'great': 5, 'world': 6, 'buffet': 7, '...': 8, 'Cine': 9, 'got': 10, 'amore': 11, 'wat': 12, 'lar': 13, 'Joking': 14, 'wif': 15, 'oni': 16, 'Free': 17, 'entry': 18, 'wkly': 19, 'comp': 20, 'win': 21, 'Cup': 22, 'final': 23, 'tkts': 24, '21st': 25, 'May': 26, '2005': 27, 'Text': 28, '87121': 29, 'receive': 30, 'question': 31, 'std': 32, 'txt': 33, 'rate': 34, 'apply': 35, '08452810075over18': 36, 'dun': 37, 'say': 38, 'early': 39, 'hor': 40, 'already': 41, 'Nah': 42, \"n't\": 43, 'think': 44, 'goes': 45, 'usf': 46, 'lives': 47, 'around': 48, 'though': 49, 'FreeMsg': 50, 'Hey': 51, 'darling': 52, 'week': 53, 'word': 54, 'back': 55, 'like': 56, 'fun': 57, 'still': 58, 'XxX': 59, 'chgs': 60, 'send': 61, '1.50': 62, 'rcv': 63, 'Even': 64, 'brother': 65, 'speak': 66, 'They': 67, 'treat': 68, 'aids': 69, 'patent': 70, 'per': 71, 'request': 72, \"'Melle\": 73, 'Melle': 74, 'Oru': 75, 'Minnaminunginte': 76, 'Nurungu': 77, 'Vettam': 78, 'set': 79, 'callertune': 80, 'Callers': 81, 'Press': 82, 'copy': 83, 'friends': 84, 'Callertune': 85, 'WINNER': 86, 'valued': 87, 'network': 88, 'customer': 89, 'selected': 90, 'receivea': 91, '900': 92, 'prize': 93, 'reward': 94, 'claim': 95, 'call': 96, '09061701461': 97, 'Claim': 98, 'code': 99, 'KL341': 100, 'Valid': 101, 'hours': 102, 'Had': 103, 'mobile': 104, 'months': 105, 'entitled': 106, 'Update': 107, 'latest': 108, 'colour': 109, 'mobiles': 110, 'camera': 111, 'Call': 112, 'The': 113, 'Mobile': 114, 'FREE': 115, '08002986030': 116, 'gon': 117, 'home': 118, 'soon': 119, 'want': 120, 'talk': 121, 'stuff': 122, 'anymore': 123, 'tonight': 124, \"'ve\": 125, 'cried': 126, 'enough': 127, 'today': 128, 'SIX': 129, 'chances': 130, 'CASH': 131, 'From': 132, '100': 133, '20,000': 134, 'pounds': 135, 'CSH11': 136, '87575': 137, 'Cost': 138, '150p/day': 139, '6days': 140, '16+': 141, 'TsandCs': 142, 'Reply': 143, 'info': 144, 'URGENT': 145, 'You': 146, 'membership': 147, '100,000': 148, 'Prize': 149, 'Jackpot': 150, 'Txt': 151, 'CLAIM': 152, '81010': 153, 'www.dbuk.net': 154, 'LCCLTD': 155, 'POBOX': 156, '4403LDNW1A7RW18': 157, 'searching': 158, 'right': 159, 'words': 160, 'thank': 161, 'breather': 162, 'promise': 163, 'wont': 164, 'take': 165, 'help': 166, 'granted': 167, 'fulfil': 168, 'wonderful': 169, 'blessing': 170, 'times': 171, 'HAVE': 172, 'DATE': 173, 'SUNDAY': 174, 'WITH': 175, 'WILL': 176, 'XXXMobileMovieClub': 177, 'use': 178, 'credit': 179, 'click': 180, 'WAP': 181, 'link': 182, 'next': 183, 'message': 184, 'http': 185, '//wap': 186, 'xxxmobilemovieclub.com': 187, 'n=QJKGIGHJJGCBL': 188, 'watching': 189, 'remember': 190, 'spell': 191, 'name': 192, 'Yes': 193, 'naughty': 194, 'make': 195, 'wet': 196, 'Fine': 197, 'thats': 198, 'way': 199, 'feel': 200, 'Thats': 201, 'gota': 202, 'England': 203, 'Macedonia': 204, 'dont': 205, 'miss': 206, 'goals/team': 207, 'news': 208, 'national': 209, 'team': 210, '87077': 211, 'ENGLAND': 212, 'Try': 213, 'WALES': 214, 'SCOTLAND': 215, '4txt/u1.20': 216, 'POBOXox36504W45WQ': 217}\n"
     ]
    }
   ],
   "source": [
    "print(test_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 8], [13, 8, 14, 15, 16, 8], [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 18, 31, 32, 33, 34, 35, 36], [37, 38, 39, 40, 8, 41, 38, 8], [42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 32, 60, 61, 62, 63], [64, 65, 56, 66, 67, 68, 56, 69, 70], [71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85], [86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102], [103, 104, 105, 106, 107, 108, 109, 110, 111, 17, 112, 113, 114, 107, 115, 116], [117, 118, 119, 43, 120, 121, 122, 123, 124, 125, 126, 127, 128], [129, 130, 21, 131, 132, 133, 134, 135, 33, 136, 61, 137, 138, 139, 140, 141, 142, 35, 143, 144], [145, 146, 53, 115, 147, 148, 149, 150, 151, 54, 152, 153, 154, 155, 156, 157], [125, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 163, 146, 169, 170, 171], [172, 173, 174, 175, 176], [177, 178, 179, 180, 181, 182, 183, 33, 184, 180, 185, 186, 187, 188], [8, 189], [190, 191, 192, 8, 193, 194, 195, 196], [197, 198, 199, 200, 201, 199, 202], [203, 204, 205, 206, 207, 208, 151, 209, 210, 211, 212, 211, 213, 214, 215, 216, 217, 141]]\n"
     ]
    }
   ],
   "source": [
    "print(test_vectorized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate frequencies of each word in the vocabulary...\n",
      "Finished calculating frequency distribution...\n"
     ]
    }
   ],
   "source": [
    "# Get frequency of each word in the vocabulary for feature engineering\n",
    "test_frequency_distribution = feature_extractor.get_word_frequency(test_vectorized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 8, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 2, 18: 2, 19: 1, 20: 1, 21: 2, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 2, 33: 3, 34: 1, 35: 2, 36: 1, 37: 1, 38: 2, 39: 1, 40: 1, 41: 1, 42: 1, 43: 2, 44: 1, 45: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1, 53: 2, 54: 2, 55: 1, 56: 3, 57: 1, 58: 1, 59: 1, 60: 1, 61: 2, 62: 1, 63: 1, 64: 1, 65: 1, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 99: 1, 100: 1, 101: 1, 102: 1, 103: 1, 104: 1, 105: 1, 106: 1, 107: 2, 108: 1, 109: 1, 110: 1, 111: 1, 112: 1, 113: 1, 114: 1, 115: 2, 116: 1, 117: 1, 118: 1, 119: 1, 120: 1, 121: 1, 122: 1, 123: 1, 124: 1, 125: 2, 126: 1, 127: 1, 128: 1, 129: 1, 130: 1, 131: 1, 132: 1, 133: 1, 134: 1, 135: 1, 136: 1, 137: 1, 138: 1, 139: 1, 140: 1, 141: 2, 142: 1, 143: 1, 144: 1, 145: 1, 146: 2, 147: 1, 148: 1, 149: 1, 150: 1, 151: 2, 152: 1, 153: 1, 154: 1, 155: 1, 156: 1, 157: 1, 158: 1, 159: 1, 160: 1, 161: 1, 162: 1, 163: 2, 164: 1, 165: 1, 166: 1, 167: 1, 168: 1, 169: 1, 170: 1, 171: 1, 172: 1, 173: 1, 174: 1, 175: 1, 176: 1, 177: 1, 178: 1, 179: 1, 180: 2, 181: 1, 182: 1, 183: 1, 184: 1, 185: 1, 186: 1, 187: 1, 188: 1, 189: 1, 190: 1, 191: 1, 192: 1, 193: 1, 194: 1, 195: 1, 196: 1, 197: 1, 198: 1, 199: 2, 200: 1, 201: 1, 202: 1, 203: 1, 204: 1, 205: 1, 206: 1, 207: 1, 208: 1, 209: 1, 210: 1, 211: 2, 212: 1, 213: 1, 214: 1, 215: 1, 216: 1, 217: 1}\n"
     ]
    }
   ],
   "source": [
    "print(test_frequency_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 120, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143, 144: 144, 145: 145, 146: 146, 147: 147, 148: 148, 149: 149, 150: 150, 151: 151, 152: 152, 153: 153, 154: 154, 155: 155, 156: 156, 157: 157, 158: 158, 159: 159, 160: 160, 161: 161, 162: 162, 163: 163, 164: 164, 165: 165, 166: 166, 167: 167, 168: 168, 169: 169, 170: 170, 171: 171, 172: 172, 173: 173, 174: 174, 175: 175, 176: 176, 177: 177, 178: 178, 179: 179, 180: 180, 181: 181, 182: 182, 183: 183, 184: 184, 185: 185, 186: 186, 187: 187, 188: 188, 189: 189, 190: 190, 191: 191, 192: 192, 193: 193, 194: 194, 195: 195, 196: 196, 197: 197, 198: 198, 199: 199, 200: 200, 201: 201, 202: 202, 203: 203, 204: 204, 205: 205, 206: 206, 207: 207, 208: 208, 209: 209, 210: 210, 211: 211, 212: 212, 213: 213, 214: 214, 215: 215, 216: 216, 217: 217}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top words in the vocabulary (threshold_value is used for band pass filtering, ie to remove low frequency words)\n",
    "test_threshold_value = 0\n",
    "test_top_words=feature_extractor.get_top_words(test_frequency_distribution, test_threshold_value) # get_top_words returns a dict top words with key-> new index, value -> index of word in the vocabulary\n",
    "print(test_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jurong': 1, 'point': 1, 'crazy..': 1, 'Available': 1, 'bugis': 1, 'great': 1, 'world': 1, 'buffet': 1, '...': 8, 'Cine': 1, 'got': 1, 'amore': 1, 'wat': 1, 'lar': 1, 'Joking': 1, 'wif': 1, 'oni': 1, 'Free': 2, 'entry': 2, 'wkly': 1, 'comp': 1, 'win': 2, 'Cup': 1, 'final': 1, 'tkts': 1, '21st': 1, 'May': 1, '2005': 1, 'Text': 1, '87121': 1, 'receive': 1, 'question': 1, 'std': 2, 'txt': 3, 'rate': 1, 'apply': 2, '08452810075over18': 1, 'dun': 1, 'say': 2, 'early': 1, 'hor': 1, 'already': 1, 'Nah': 1, \"n't\": 2, 'think': 1, 'goes': 1, 'usf': 1, 'lives': 1, 'around': 1, 'though': 1, 'FreeMsg': 1, 'Hey': 1, 'darling': 1, 'week': 2, 'word': 2, 'back': 1, 'like': 3, 'fun': 1, 'still': 1, 'XxX': 1, 'chgs': 1, 'send': 2, '1.50': 1, 'rcv': 1, 'Even': 1, 'brother': 1, 'speak': 1, 'They': 1, 'treat': 1, 'aids': 1, 'patent': 1, 'per': 1, 'request': 1, \"'Melle\": 1, 'Melle': 1, 'Oru': 1, 'Minnaminunginte': 1, 'Nurungu': 1, 'Vettam': 1, 'set': 1, 'callertune': 1, 'Callers': 1, 'Press': 1, 'copy': 1, 'friends': 1, 'Callertune': 1, 'WINNER': 1, 'valued': 1, 'network': 1, 'customer': 1, 'selected': 1, 'receivea': 1, '900': 1, 'prize': 1, 'reward': 1, 'claim': 1, 'call': 1, '09061701461': 1, 'Claim': 1, 'code': 1, 'KL341': 1, 'Valid': 1, 'hours': 1, 'Had': 1, 'mobile': 1, 'months': 1, 'entitled': 1, 'Update': 2, 'latest': 1, 'colour': 1, 'mobiles': 1, 'camera': 1, 'Call': 1, 'The': 1, 'Mobile': 1, 'FREE': 2, '08002986030': 1, 'gon': 1, 'home': 1, 'soon': 1, 'want': 1, 'talk': 1, 'stuff': 1, 'anymore': 1, 'tonight': 1, \"'ve\": 2, 'cried': 1, 'enough': 1, 'today': 1, 'SIX': 1, 'chances': 1, 'CASH': 1, 'From': 1, '100': 1, '20,000': 1, 'pounds': 1, 'CSH11': 1, '87575': 1, 'Cost': 1, '150p/day': 1, '6days': 1, '16+': 2, 'TsandCs': 1, 'Reply': 1, 'info': 1, 'URGENT': 1, 'You': 2, 'membership': 1, '100,000': 1, 'Prize': 1, 'Jackpot': 1, 'Txt': 2, 'CLAIM': 1, '81010': 1, 'www.dbuk.net': 1, 'LCCLTD': 1, 'POBOX': 1, '4403LDNW1A7RW18': 1, 'searching': 1, 'right': 1, 'words': 1, 'thank': 1, 'breather': 1, 'promise': 2, 'wont': 1, 'take': 1, 'help': 1, 'granted': 1, 'fulfil': 1, 'wonderful': 1, 'blessing': 1, 'times': 1, 'HAVE': 1, 'DATE': 1, 'SUNDAY': 1, 'WITH': 1, 'WILL': 1, 'XXXMobileMovieClub': 1, 'use': 1, 'credit': 1, 'click': 2, 'WAP': 1, 'link': 1, 'next': 1, 'message': 1, 'http': 1, '//wap': 1, 'xxxmobilemovieclub.com': 1, 'n=QJKGIGHJJGCBL': 1, 'watching': 1, 'remember': 1, 'spell': 1, 'name': 1, 'Yes': 1, 'naughty': 1, 'make': 1, 'wet': 1, 'Fine': 1, 'thats': 1, 'way': 2, 'feel': 1, 'Thats': 1, 'gota': 1, 'England': 1, 'Macedonia': 1, 'dont': 1, 'miss': 1, 'goals/team': 1, 'news': 1, 'national': 1, 'team': 1, '87077': 2, 'ENGLAND': 1, 'Try': 1, 'WALES': 1, 'SCOTLAND': 1, '4txt/u1.20': 1, 'POBOXox36504W45WQ': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create a dic of top words and their frequency distribution to visualize using wordcloud\n",
    "test_word_freq = {}\n",
    "for value in test_top_words.values():\n",
    "    key = feature_extractor.get_key(test_vocabulary,value)\n",
    "    test_word_freq[key] = test_frequency_distribution[value]\n",
    "print(test_word_freq)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file to reports\n",
    "wordcloud.to_file(\"../reports/figures/word_count.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words\n",
    "dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort word frequency dictionary for analysis\n",
    "sorted_d = sorted(word_freq.items(), key=lambda x: x[1])\n",
    "print(sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a vocabulary of all words and a top words in the entire dataset\n",
    "# And we need to obtain a feature vector by encoding these words into numerical vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Techniques for Encoding - All the popular techniques that are used for encoding.\n",
    "    * Bag of Words\n",
    "    * Binary Bag of Words\n",
    "    * Bigram, Ngram\n",
    "    * TF-IDF( Term Frequency - Inverse Document Frequency)\n",
    "    * Word2Vec\n",
    "    * Avg-Word2Vec\n",
    "    * TF-IDF Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bag of Words Model — \n",
    "    * Find the unique words i.e., vocabulary from the list of documents. \n",
    "    * Parse each document word with the vocabulary, if present ‘1’ else ‘0’. \n",
    "    * This makes each document vector maintain the same length that of vocabulary length.\n",
    "    * We use this vocabulary for the new document vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency of each word in the vocabulary for feature engineering\n",
    "from src.functions import feature_vector\n",
    "#print({k: vocabulary[k] for k in list(vocabulary)[:500]})\n",
    "binary_bow = feature_vector.custom_binary_bow_vector(vocabulary,tokenized_dataset['sms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Counts with CountVectorizer(scikit-learn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BINARY BAG OF WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In binary BoW, we dont count the frequency of word, we just place 1 if the word appears in the review or else 0. In CountVectorizer there is a parameter binary = true this makes our BoW to binary BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_count_vectorizer_vector = feature_vector.binary_count_vectorizer(preprocessor.preprocess,cleaned_dataset['sms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_count_vectorizer_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drawbacks of BoW/ Binary BoW\n",
    "\n",
    "Our main objective in doing these text to vector encodings is that similar meaning text vectors should be close to each other, but in some cases this may not possible for Bow\n",
    "\n",
    "For example, if we consider two reviews This pasta is very tasty and This pasta is not tasty after stopwords removal both sentences will be converted to pasta tasty so both giving exact same meaning.\n",
    "\n",
    "The main problem is here we are not considering the front and back words related to every word, here comes Bigram and Ngram techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BI-GRAM BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering pair of words for creating dictionary is Bi-Gram , Tri-Gram means three consecutive words so as NGram.\n",
    "\n",
    "CountVectorizer has a parameter ngram_range if assigned to (1,2) it considers Bi-Gram BoW\n",
    "\n",
    "But this massively increases our dictionary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vectorizer_vector = feature_vector.bigram_count_vectorizer(preprocessor.preprocess,cleaned_dataset['sms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequencies with TfidfVectorizer (scikit-learn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency it makes sure that less importance is given to most frequent words and also considers less frequent words.\n",
    "\n",
    "Term Frequency is number of times a particular word(W) occurs in a review divided by totall number of words (Wr) in review. The term frequency value ranges from 0 to 1.\n",
    "\n",
    "Inverse Document Frequency is calculated as log(Total Number of Docs(N) / Number of Docs which contains particular word(n)). Here Docs referred as Reviews.\n",
    "\n",
    "TF-IDF is TF * IDF that is (W/Wr)*LOG(N/n)\n",
    "\n",
    "Using scikit-learn's tfidfVectorizer we can get the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = feature_vector.tfidf_vectorizer(preprocessor.preprocess,cleaned_dataset['sms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf is the best vectorization method among these three, because it prioritise the words in each document. IDF value for the word “this” is less since it present in both the documents. So, unlike word counts which give higher value for stop words like “in”, “this”, word frequency lowers the value if it present in more number of documents, because stop words repeats in each document almost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Limitaions of TFIDF:\n",
    "\n",
    "So even here we get a TF-IDF value for every word and in some cases it may consider different meaning reviews as similar after stopwords removal. so to over come we can use BI-Gram or NGram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to actually overcome the problem of semantical reviews having close distance we have Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec actually takes the semantic meaning of the words and their relationships between other words. it learns all the internal relationships between the words.It represents the word in dense vector form.\n",
    "\n",
    "Using Gensim's library we have Word2Vec which takes parameters like min_count = 5 considers only if word repeats more than 5 times in entire data. size = 50 gives a vector length of size 50 and workers are cores to run this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Word2vec of each of the words and add the vectors of each words of the sentence and divide the vector with the number of words of the sentence.Simply Averaging the Word2Vec of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vector = feature_vector.word2vec(preprocessor.preprocess,tokenized_dataset['sms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_vector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F-IDF WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TF-IDF Word2Vec the Word2Vec value of each word is multiplied by the tfidf value of that word and summed up and then divided by the sum of the tfidf values of the sentence.\n",
    "\n",
    "   V = ( t(W1)*w2v(W1) + t(W2)*w2v(W2) +.....+t(Wn)*w2v(Wn))/(t(W1)+t(W2)+....+t(Wn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_tfidf_vectorizer = feature_vector.word2vec_tfidf_vectorizer(word2vec_vector,cleaned_dataset['sms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_tfidf_vectorizer[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CountVectorizer for Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=preprocessor.preprocess)\n",
    "bow_transformer.fit(sms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a vocabulary of all words and a top words in the entire dataset\n",
    "# And we need to obtain a feature vector by encoding these words into numerical vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Techniques for Encoding - All the popular techniques that are used for encoding.\n",
    "    * Bag of Words\n",
    "    * Binary Bag of Words\n",
    "    * Bigram, Ngram\n",
    "    * TF-IDF( Term Frequency - Inverse Document Frequency)\n",
    "    * Word2Vec\n",
    "    * Avg-Word2Vec\n",
    "    * TF-IDF Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message4 = sms_df[3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (bow_transformer.get_feature_names()[6043])\n",
    "print (bow_transformer.get_feature_names()[2016])\n",
    "print (bow_transformer.get_feature_names()[2035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(sms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print ('Amount of Non-Zero occurences: ', messages_bow.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * messages_bow.nnz /\n",
    "                             (messages_bow.shape[0] * messages_bow.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 2\n",
    "km = KMeans(num_clusters,random_state=99,init='k-means++', n_init=14, max_iter=100, tol=0.00001, copy_x=True)\n",
    "km.fit(messages_tfidf)\n",
    "clusters = km.labels_.tolist()\n",
    "print(\"Results of Clustering:\")\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
